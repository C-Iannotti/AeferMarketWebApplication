{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7de3cf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Invoice ID Branch       City Customer type  Gender  \\\n",
      "0    750-67-8428      A     Yangon        Member  Female   \n",
      "1    226-31-3081      C  Naypyitaw        Normal  Female   \n",
      "2    631-41-3108      A     Yangon        Normal    Male   \n",
      "3    123-19-1176      A     Yangon        Member    Male   \n",
      "4    373-73-7910      A     Yangon        Normal    Male   \n",
      "..           ...    ...        ...           ...     ...   \n",
      "995  233-67-5758      C  Naypyitaw        Normal    Male   \n",
      "996  303-96-2227      B   Mandalay        Normal  Female   \n",
      "997  727-02-1313      A     Yangon        Member    Male   \n",
      "998  347-56-2442      A     Yangon        Normal    Male   \n",
      "999  849-09-3807      A     Yangon        Member  Female   \n",
      "\n",
      "               Product line  Unit price  Quantity   Tax 5%      Total  \\\n",
      "0         Health and beauty       74.69         7  26.1415   548.9715   \n",
      "1    Electronic accessories       15.28         5   3.8200    80.2200   \n",
      "2        Home and lifestyle       46.33         7  16.2155   340.5255   \n",
      "3         Health and beauty       58.22         8  23.2880   489.0480   \n",
      "4         Sports and travel       86.31         7  30.2085   634.3785   \n",
      "..                      ...         ...       ...      ...        ...   \n",
      "995       Health and beauty       40.35         1   2.0175    42.3675   \n",
      "996      Home and lifestyle       97.38        10  48.6900  1022.4900   \n",
      "997      Food and beverages       31.84         1   1.5920    33.4320   \n",
      "998      Home and lifestyle       65.82         1   3.2910    69.1110   \n",
      "999     Fashion accessories       88.34         7  30.9190   649.2990   \n",
      "\n",
      "          Date   Time      Payment    cogs  gross margin percentage  \\\n",
      "0     1/5/2019  13:08      Ewallet  522.83                 4.761905   \n",
      "1     3/8/2019  10:29         Cash   76.40                 4.761905   \n",
      "2     3/3/2019  13:23  Credit card  324.31                 4.761905   \n",
      "3    1/27/2019  20:33      Ewallet  465.76                 4.761905   \n",
      "4     2/8/2019  10:37      Ewallet  604.17                 4.761905   \n",
      "..         ...    ...          ...     ...                      ...   \n",
      "995  1/29/2019  13:46      Ewallet   40.35                 4.761905   \n",
      "996   3/2/2019  17:16      Ewallet  973.80                 4.761905   \n",
      "997   2/9/2019  13:22         Cash   31.84                 4.761905   \n",
      "998  2/22/2019  15:33         Cash   65.82                 4.761905   \n",
      "999  2/18/2019  13:28         Cash  618.38                 4.761905   \n",
      "\n",
      "     gross income  Rating  \n",
      "0         26.1415     9.1  \n",
      "1          3.8200     9.6  \n",
      "2         16.2155     7.4  \n",
      "3         23.2880     8.4  \n",
      "4         30.2085     5.3  \n",
      "..            ...     ...  \n",
      "995        2.0175     6.2  \n",
      "996       48.6900     4.4  \n",
      "997        1.5920     7.7  \n",
      "998        3.2910     4.1  \n",
      "999       30.9190     6.6  \n",
      "\n",
      "[1000 rows x 17 columns]\n",
      "    Branch            Product line       Date  Quantity\n",
      "0        A       Health and beauty 2019-01-05         7\n",
      "1        C  Electronic accessories 2019-03-08         5\n",
      "2        A      Home and lifestyle 2019-03-03         7\n",
      "3        A       Health and beauty 2019-01-27         8\n",
      "4        A       Sports and travel 2019-02-08         7\n",
      "..     ...                     ...        ...       ...\n",
      "995      C       Health and beauty 2019-01-29         1\n",
      "996      B      Home and lifestyle 2019-03-02        10\n",
      "997      A      Food and beverages 2019-02-09         1\n",
      "998      A      Home and lifestyle 2019-02-22         1\n",
      "999      A     Fashion accessories 2019-02-18         7\n",
      "\n",
      "[1000 rows x 4 columns]\n",
      "     Branch            Product line       Date  Quantity\n",
      "0         A  Electronic accessories 2019-01-01        10\n",
      "1         A  Electronic accessories 2019-01-02         0\n",
      "2         A  Electronic accessories 2019-01-03         0\n",
      "3         A  Electronic accessories 2019-01-04         0\n",
      "4         A  Electronic accessories 2019-01-05         7\n",
      "...     ...                     ...        ...       ...\n",
      "1597      C       Sports and travel 2019-03-26         0\n",
      "1598      C       Sports and travel 2019-03-27         0\n",
      "1599      C       Sports and travel 2019-03-28         0\n",
      "1600      C       Sports and travel 2019-03-29         0\n",
      "1601      C       Sports and travel 2019-03-30         0\n",
      "\n",
      "[1602 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "data = pd.read_csv(\"../../data/supermarket_sales - Sheet1.csv\")\n",
    "print(data)\n",
    "\n",
    "data = data[[\"Branch\", \"Product line\", \"Date\", \"Quantity\"]]\n",
    "data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
    "print(data)\n",
    "\n",
    "min_date = data[\"Date\"].min()\n",
    "max_date = data[\"Date\"].max()\n",
    "delta = max_date - min_date\n",
    "\n",
    "for i in range(delta.days + 1):\n",
    "    day = min_date + timedelta(days=i)\n",
    "    for branch in pd.unique(data[\"Branch\"]):\n",
    "        for product_line in pd.unique(data[\"Product line\"]):\n",
    "            data.loc[len(data)] = [branch, product_line, day, 0]\n",
    "data = data.groupby([\"Branch\", \"Product line\", \"Date\"])[\"Quantity\"].sum().reset_index()\n",
    "print(data)\n",
    "data.to_pickle(\"model_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c446a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Branch            Product line       Date  Quantity\n",
      "0         A  Electronic accessories 2019-01-01        10\n",
      "1         A  Electronic accessories 2019-01-02         0\n",
      "2         A  Electronic accessories 2019-01-03         0\n",
      "3         A  Electronic accessories 2019-01-04         0\n",
      "4         A  Electronic accessories 2019-01-05         7\n",
      "...     ...                     ...        ...       ...\n",
      "1597      C       Sports and travel 2019-03-26         0\n",
      "1598      C       Sports and travel 2019-03-27         0\n",
      "1599      C       Sports and travel 2019-03-28         0\n",
      "1600      C       Sports and travel 2019-03-29         0\n",
      "1601      C       Sports and travel 2019-03-30         0\n",
      "\n",
      "[1602 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_pickle(\"./model_data.pkl\")\n",
    "print(data)\n",
    "\n",
    "train_data = pd.DataFrame(columns=[\"Branch\", \"Product line\", \"Month\"] + [f\"{i}\" for i in range(14)] + [\"Class\"])\n",
    "branches = pd.unique(data[\"Branch\"])\n",
    "product_lines = pd.unique(data[\"Product line\"])\n",
    "j=0\n",
    "\n",
    "for [branch, product_line], cur_frame in data.groupby([\"Branch\", \"Product line\"]):\n",
    "    values = cur_frame[\"Quantity\"]\n",
    "    cur_slide = values.iloc[:14].to_list()\n",
    "    for i in range(14, values.shape[0]):\n",
    "        label = 1\n",
    "\n",
    "        if cur_slide[-1] - values.iloc[i] >= cur_slide[-1] * 0.33:\n",
    "            label = 0\n",
    "        elif cur_slide[-1] - values.iloc[i] <= cur_slide[-1] * -0.33:\n",
    "            label = 2\n",
    "        train_data.loc[len(train_data.index)] = [branch, product_line, cur_frame.iloc[i][\"Date\"].month] + cur_slide + [label]\n",
    "\n",
    "        cur_slide.pop(0)\n",
    "        cur_slide.append(values.iloc[i])\n",
    "            \n",
    "train_data = pd.concat([train_data, pd.get_dummies(train_data[\"Branch\"])], axis=1)\n",
    "train_data = pd.concat([train_data, pd.get_dummies(train_data[\"Product line\"])], axis=1)\n",
    "\n",
    "train_data.pop(\"Branch\")\n",
    "train_data.pop(\"Product line\")\n",
    "train_data = train_data[[c for c in train_data if c not in [f\"{i}\" for i in range(14)]] \n",
    "       + [f\"{i}\" for i in range(14)]]\n",
    "            \n",
    "#print(train_data)\n",
    "train_data_columns = train_data.iloc[:0].copy().drop(columns=[\"Class\"])\n",
    "#print(train_data_columns)\n",
    "train_data_columns.to_pickle(\"transformed_model_columns.pkl\")\n",
    "train_data.to_pickle(\"transformed_model_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5591e96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "0.05185185185185185\n",
      "0.34814814814814815\n",
      "\n",
      "0.39187643020594964\n",
      "0.37757437070938216\n",
      "0.2305491990846682\n",
      "\n",
      "0.3959731543624161\n",
      "0.3624161073825503\n",
      "0.24161073825503357\n",
      "\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_pickle(\"./transformed_model_data.pkl\")\n",
    "#print(data)\n",
    "train_data = data.sample(frac=0.85)\n",
    "valid_data = data.drop(train_data.index)\n",
    "test_data = valid_data.sample(frac=0.5)\n",
    "valid_data = valid_data.drop(test_data.index)\n",
    "\n",
    "train_0 = train_data[train_data[\"Class\"] == 0]\n",
    "train_1 = train_data[train_data[\"Class\"] == 1]\n",
    "train_2 = train_data[train_data[\"Class\"] == 2]\n",
    "max_train = (train_0 if train_0.shape[0] >= train_1.shape[0] and train_0.shape[0] >= train_2.shape[0] else\n",
    "             train_1 if train_1.shape[0] >= train_0.shape[0] and train_1.shape[0] >= train_2.shape[0] else\n",
    "             train_2)\n",
    "\n",
    "valid_0 = valid_data[valid_data[\"Class\"] == 0]\n",
    "valid_1 = valid_data[valid_data[\"Class\"] == 1]\n",
    "valid_2 = valid_data[valid_data[\"Class\"] == 2]\n",
    "max_valid = (valid_0 if valid_0.shape[0] >= valid_1.shape[0] and valid_0.shape[0] >= valid_2.shape[0] else\n",
    "             valid_1 if valid_1.shape[0] >= valid_0.shape[0] and valid_1.shape[0] >= valid_2.shape[0] else\n",
    "             valid_2)\n",
    "\n",
    "for data_item in [train_0, train_1, train_2]:\n",
    "    cur_num = data_item.shape[0]\n",
    "    while cur_num + data_item.shape[0] < max_train.shape[0]:\n",
    "        train_data = pd.concat([train_data, data_item])\n",
    "        cur_num += data_item.shape[0]\n",
    "\n",
    "for data_item in [valid_0, valid_1, valid_2]:\n",
    "    cur_num = data_item.shape[0]\n",
    "    while cur_num + data_item.shape[0] < max_valid.shape[0]:\n",
    "        valid_data = pd.concat([valid_data, data_item])\n",
    "        cur_num += data_item.shape[0]\n",
    "        \n",
    "print((data[\"Class\"] == 0).sum() / data.shape[0])\n",
    "print((data[\"Class\"] == 1).sum() / data.shape[0])\n",
    "print((data[\"Class\"] == 2).sum() / data.shape[0])\n",
    "print()\n",
    "\n",
    "print((train_data[\"Class\"] == 0).sum() / train_data.shape[0])\n",
    "print((train_data[\"Class\"] == 1).sum() / train_data.shape[0])\n",
    "print((train_data[\"Class\"] == 2).sum() / train_data.shape[0])\n",
    "print()\n",
    "\n",
    "print((valid_data[\"Class\"] == 0).sum() / valid_data.shape[0])\n",
    "print((valid_data[\"Class\"] == 1).sum() / valid_data.shape[0])\n",
    "print((valid_data[\"Class\"] == 2).sum() / valid_data.shape[0])\n",
    "print()\n",
    "\n",
    "print(pd.merge(train_data, valid_data).shape[0])\n",
    "print()\n",
    "\n",
    "train_data.to_pickle(\"train_data.pkl\")\n",
    "valid_data.to_pickle(\"valid_data.pkl\")\n",
    "test_data.to_pickle(\"test_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "77c91081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "7/7 [==============================] - 2s 6ms/step - loss: 0.9581 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.9581\n",
      "Epoch 2/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6657 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.6657\n",
      "Epoch 3/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6074 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.6074\n",
      "Epoch 4/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.5792 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.5792\n",
      "Epoch 5/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.5551 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.5551\n",
      "Epoch 6/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.5363 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.5363\n",
      "Epoch 7/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.5202 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.5202\n",
      "Epoch 8/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.5092 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.5092\n",
      "Epoch 9/40\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.4933 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.4933\n",
      "Epoch 10/40\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.4802 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.4802\n",
      "Epoch 11/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.4745 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.4745\n",
      "Epoch 12/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.4600 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.4600\n",
      "Epoch 13/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.4495 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.4495\n",
      "Epoch 14/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.4400 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.4400\n",
      "Epoch 15/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.4317 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.4317\n",
      "Epoch 16/40\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4264 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.4264\n",
      "Epoch 17/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.4126 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.4126\n",
      "Epoch 18/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.4060 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.4060\n",
      "Epoch 19/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3939 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.3939\n",
      "Epoch 20/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3903 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.3903\n",
      "Epoch 21/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3806 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.3806\n",
      "Epoch 22/40\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3720 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.3720\n",
      "Epoch 23/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3651 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.3651\n",
      "Epoch 24/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3614 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.3614\n",
      "Epoch 25/40\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.3501 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.3501\n",
      "Epoch 26/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3449 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.3449\n",
      "Epoch 27/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3385 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.3385\n",
      "Epoch 28/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3304 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.3304\n",
      "Epoch 29/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3206 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.3206\n",
      "Epoch 30/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3176 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.3176\n",
      "Epoch 31/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3101 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.3101\n",
      "Epoch 32/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3078 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.3078\n",
      "Epoch 33/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2958 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.2958\n",
      "Epoch 34/40\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2920 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.2920\n",
      "Epoch 35/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2880 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.2880\n",
      "Epoch 36/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2822 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.2822\n",
      "Epoch 37/40\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2725 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.2725\n",
      "Epoch 38/40\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2686 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.2686\n",
      "Epoch 39/40\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2659 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.2659\n",
      "Epoch 40/40\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2553 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.2553\n",
      "Training Accuracy:  0.7013729977116705\n",
      "Valid Accuracy:  0.4563758389261745\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(seed=42)\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_data_pd = pd.read_pickle(\"./train_data.pkl\")\n",
    "valid_data_pd = pd.read_pickle(\"./valid_data.pkl\")\n",
    "train_class_pd = train_data_pd.pop(\"Class\")\n",
    "valid_class_pd = valid_data_pd.pop(\"Class\")\n",
    "\n",
    "input_data = tf.convert_to_tensor(train_data_pd.values.reshape(-1, 1, train_data_pd.shape[1]))\n",
    "input_labels = tf.convert_to_tensor(train_class_pd.values.reshape(-1, 1, 1), dtype=tf.float32)\n",
    "valid_data = tf.convert_to_tensor(valid_data_pd.values.reshape(-1, 1, valid_data_pd.shape[1]))\n",
    "valid_labels = tf.convert_to_tensor(valid_class_pd.values.reshape(-1, 1, 1), dtype=tf.float32)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    layers.LSTM(units=30, input_shape=(1, train_data_pd.shape[1]), return_sequences=True),\n",
    "    layers.Dense(units=1),\n",
    "    layers.Reshape([1,-1])\n",
    "])\n",
    "loss = keras.losses.MeanSquaredError()\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.0025)\n",
    "metrics = [keras.metrics.CategoricalCrossentropy(), keras.metrics.MeanSquaredError()]\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=metrics\n",
    ")\n",
    "model.fit(x=input_data, y=input_labels, epochs=40, batch_size=BATCH_SIZE)\n",
    "print(\"Training Accuracy: \", float(tf.divide(tf.reduce_sum(tf.cast(tf.equal(tf.round(model(input_data)), input_labels), tf.int32)), tf.shape(input_labels)[0])))\n",
    "print(\"Valid Accuracy: \", float(tf.divide(tf.reduce_sum(tf.cast(tf.equal(tf.round(model(valid_data)), valid_labels), tf.int32)), tf.shape(valid_labels)[0])))\n",
    "\n",
    "model.save(\"./model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "68b94ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 2s 4ms/step - loss: 0.0780 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.0780\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0783 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.0783\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0619 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.0619\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0878 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.0878\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0584 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.0584\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0713 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.0713\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0709 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.0709\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0617 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.0617\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0658 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.0658\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0586 - categorical_crossentropy: 9.9978e-08 - mean_squared_error: 0.0586\n",
      "Training Accuracy:  0.9296338672768879\n",
      "Valid Accuracy:  0.3422818791946309\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(seed=42)\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_data_pd = pd.read_pickle(\"./train_data.pkl\")\n",
    "valid_data_pd = pd.read_pickle(\"./valid_data.pkl\")\n",
    "train_class_pd = train_data_pd.pop(\"Class\")\n",
    "valid_class_pd = valid_data_pd.pop(\"Class\")\n",
    "\n",
    "input_data = tf.convert_to_tensor(train_data_pd.values.reshape(-1, 1, train_data_pd.shape[1]))\n",
    "input_labels = tf.convert_to_tensor(train_class_pd.values.reshape(-1, 1, 1), dtype=tf.float32)\n",
    "valid_data = tf.convert_to_tensor(valid_data_pd.values.reshape(-1, 1, valid_data_pd.shape[1]))\n",
    "valid_labels = tf.convert_to_tensor(valid_class_pd.values.reshape(-1, 1, 1), dtype=tf.float32)\n",
    "\n",
    "model = keras.models.load_model(\"./model.h5\")\n",
    "model.fit(x=input_data, y=input_labels, epochs=10, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"Training Accuracy: \", float(tf.divide(tf.reduce_sum(tf.cast(tf.equal(tf.round(model(input_data)), input_labels), tf.int32)), tf.shape(input_labels)[0])))\n",
    "print(\"Valid Accuracy: \", float(tf.divide(tf.reduce_sum(tf.cast(tf.equal(tf.round(model(valid_data)), valid_labels), tf.int32)), tf.shape(valid_labels)[0])))\n",
    "\n",
    "model.save(\"./model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d9d41d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 0: 0.7586206896551724\n",
      "Recall 0: 0.3333333333333333\n",
      "Precision 1: 0.01694915254237288\n",
      "Recall 1: 0.25\n",
      "Precision 2: 0.5\n",
      "Recall 2: 0.16129032258064516\n",
      "Accuracy:  0.27722772277227725\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "test_data_pd = pd.read_pickle(\"./test_data.pkl\")\n",
    "test_class_pd = test_data_pd.pop(\"Class\")\n",
    "\n",
    "test_data = tf.convert_to_tensor(test_data_pd.values.reshape(-1, 1, test_data_pd.shape[1]))\n",
    "test_labels = tf.convert_to_tensor(test_class_pd.values.reshape(-1, 1, 1), dtype=tf.float32)\n",
    "\n",
    "model = keras.models.load_model(\"./model.h5\")\n",
    "\n",
    "predictions = tf.round(model(test_data))\n",
    "\n",
    "equals_0 = float(tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(predictions, test_labels), tf.equal(test_labels, 0)), tf.int32)))\n",
    "missed_0 = float(tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(test_labels, 0), tf.not_equal(predictions, 0)), tf.int32)))\n",
    "guessed_0 = float(tf.reduce_sum(tf.cast(tf.logical_and(tf.not_equal(test_labels, 0), tf.equal(predictions, 0)), tf.int32)))\n",
    "\n",
    "equals_1 = float(tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(predictions, test_labels), tf.equal(test_labels, 1)), tf.int32)))\n",
    "missed_1 = float(tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(test_labels, 1), tf.not_equal(predictions, 1)), tf.int32)))\n",
    "guessed_1 = float(tf.reduce_sum(tf.cast(tf.logical_and(tf.not_equal(test_labels, 1), tf.equal(predictions, 1)), tf.int32)))\n",
    "\n",
    "equals_2 = float(tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(predictions, test_labels), tf.equal(test_labels, 2)), tf.int32)))\n",
    "missed_2 = float(tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(test_labels, 2), tf.not_equal(predictions, 2)), tf.int32)))\n",
    "guessed_2 = float(tf.reduce_sum(tf.cast(tf.logical_and(tf.not_equal(test_labels, 2), tf.equal(predictions, 2)), tf.int32)))\n",
    "\n",
    "print(\"Precision 0:\", equals_0 / (equals_0 + guessed_0))\n",
    "print(\"Recall 0:\", equals_0 / (equals_0 + missed_0))\n",
    "print(\"Precision 1:\", equals_1 / (equals_1 + guessed_1))\n",
    "print(\"Recall 1:\", equals_1 / (equals_1 + missed_1))\n",
    "print(\"Precision 2:\", equals_2 / (equals_2 + guessed_2))\n",
    "print(\"Recall 2:\", equals_2 / (equals_2 + missed_2))\n",
    "print(\"Accuracy: \", float(tf.divide(tf.reduce_sum(tf.cast(tf.equal(predictions, test_labels), tf.int32)), tf.shape(test_labels)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c25ebb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "7/7 [==============================] - 2s 5ms/step - loss: 1.0530 - categorical_crossentropy: 9.8604e-08 - mean_squared_error: 1.0530\n",
      "Epoch 2/25\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.8974 - categorical_crossentropy: 9.8604e-08 - mean_squared_error: 0.8974\n",
      "Epoch 3/25\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.8701 - categorical_crossentropy: 9.8604e-08 - mean_squared_error: 0.8701\n",
      "Epoch 4/25\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.8188 - categorical_crossentropy: 9.8604e-08 - mean_squared_error: 0.8188\n",
      "Epoch 5/25\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.8155 - categorical_crossentropy: 9.8604e-08 - mean_squared_error: 0.8155\n",
      "Epoch 6/25\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.7942 - categorical_crossentropy: 9.8604e-08 - mean_squared_error: 0.7942\n",
      "Epoch 7/25\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.7879 - categorical_crossentropy: 9.8604e-08 - mean_squared_error: 0.7879\n",
      "Epoch 8/25\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.7806 - categorical_crossentropy: 9.8604e-08 - mean_squared_error: 0.7806\n",
      "Epoch 9/25\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.7770 - categorical_crossentropy: 9.8604e-08 - mean_squared_error: 0.7770\n",
      "Epoch 10/25\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.7222 - categorical_crossentropy: 9.8604e-08 - mean_squared_error: 0.7222\n",
      "Epoch 11/25\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.7357 - categorical_crossentropy: 9.8604e-08 - mean_squared_error: 0.7357\n",
      "Epoch 12/25\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.7274 - categorical_crossentropy: 9.8604e-08 - mean_squared_error: 0.7274\n",
      "Epoch 13/25\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.7046 - categorical_crossentropy: 9.8604e-08 - mean_squared_error: 0.7046\n",
      "Epoch 14/25\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6970 - categorical_crossentropy: 9.8604e-08 - mean_squared_error: 0.6970\n",
      "Epoch 15/25\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.7068 - categorical_crossentropy: 9.8604e-08 - mean_squared_error: 0.7068\n",
      "Epoch 16/25\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6869 - categorical_crossentropy: 9.8604e-08 - mean_squared_error: 0.6869\n",
      "Epoch 17/25\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6699 - categorical_crossentropy: 9.8604e-08 - mean_squared_error: 0.6699\n",
      "Epoch 18/25\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6754 - categorical_crossentropy: 9.8604e-08 - mean_squared_error: 0.6754\n",
      "Epoch 19/25\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6626 - categorical_crossentropy: 9.8604e-08 - mean_squared_error: 0.6626\n",
      "Epoch 20/25\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6532 - categorical_crossentropy: 9.8604e-08 - mean_squared_error: 0.6532\n",
      "Epoch 21/25\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6648 - categorical_crossentropy: 9.8604e-08 - mean_squared_error: 0.6648\n",
      "Epoch 22/25\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6404 - categorical_crossentropy: 9.8604e-08 - mean_squared_error: 0.6404\n",
      "Epoch 23/25\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6461 - categorical_crossentropy: 9.8604e-08 - mean_squared_error: 0.6461\n",
      "Epoch 24/25\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6542 - categorical_crossentropy: 9.8604e-08 - mean_squared_error: 0.6542\n",
      "Epoch 25/25\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6579 - categorical_crossentropy: 9.8604e-08 - mean_squared_error: 0.6579\n",
      "Training Accuracy:  0.3857308584686775\n",
      "Valid Accuracy:  0.39867109634551495\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from collections import OrderedDict\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(seed=42)\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_ds = tf.data.experimental.make_csv_dataset(\n",
    "    \"train_data.csv\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=1,\n",
    "    ignore_errors=True,\n",
    "    label_name=\"Class\"\n",
    ")\n",
    "\n",
    "valid_ds = tf.data.experimental.make_csv_dataset(\n",
    "    \"valid_data.csv\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=1,\n",
    "    ignore_errors=True,\n",
    "    label_name=\"Class\"\n",
    ")\n",
    "\n",
    "def map_func(x, class_):\n",
    "    od_ = OrderedDict()\n",
    "    od_[\"lstm_input\"] = tf.reshape(tf.stack(list(x.values())), (-1, 1, len(list(x.values()))))\n",
    "    class_ = tf.reshape(class_, (-1, 1, 1))\n",
    "    return (od_, class_)\n",
    "\n",
    "train_ds = train_ds.map(map_func)\n",
    "valid_ds = valid_ds.map(map_func)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    layers.LSTM(units=30, input_shape=(1, 24), return_sequences=True, name=\"lstm\"),\n",
    "    layers.Dense(units=1),\n",
    "    layers.Reshape([1,-1])\n",
    "])\n",
    "loss = keras.losses.MeanSquaredError()\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.0005)\n",
    "metrics = [keras.metrics.CategoricalCrossentropy(), keras.metrics.MeanSquaredError()]\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=metrics\n",
    ")\n",
    "model.fit(x=train_ds, epochs=25, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "num_accur_train = 0\n",
    "total_train = 0\n",
    "for batch in train_ds:\n",
    "    res = model(batch[0][\"lstm_input\"])\n",
    "    num_accur_train += int(tf.reduce_sum(tf.cast(tf.equal(tf.cast(tf.round(res), tf.int32), batch[1]), tf.int32)))\n",
    "    total_train += int(tf.shape(batch[1])[0])\n",
    "\n",
    "num_accur_valid = 0\n",
    "total_valid = 0\n",
    "for batch in valid_ds:\n",
    "    res = model(batch[0][\"lstm_input\"])\n",
    "    num_accur_valid += int(tf.reduce_sum(tf.cast(tf.equal(tf.cast(tf.round(res), tf.int32), batch[1]), tf.int32)))\n",
    "    total_valid += int(tf.shape(batch[1])[0])\n",
    "\n",
    "print(\"Training Accuracy: \", num_accur_train / total_train)\n",
    "print(\"Valid Accuracy: \", num_accur_valid / total_valid)\n",
    "\n",
    "model.save(\"./model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8cae4f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 24 120], shape=(2,), dtype=int32)\n",
      "tf.Tensor([ 30 120], shape=(2,), dtype=int32)\n",
      "tf.Tensor([120], shape=(1,), dtype=int32)\n",
      "tf.Tensor([30  1], shape=(2,), dtype=int32)\n",
      "tf.Tensor([1], shape=(1,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from collections import OrderedDict\n",
    "\n",
    "model = keras.models.load_model(\"./model.h5\")\n",
    "weights = model.get_weights()\n",
    "for weight in weights:\n",
    "    #print(weight)\n",
    "    print(tf.shape(weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e833f458",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9e4189a0944fa5bc86c43b5208041d82fa51ec442dbf1706f6002494df53cf7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
